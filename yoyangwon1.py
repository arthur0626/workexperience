import time, requests
from bs4 import BeautifulSoup
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from webdriver_manager.chrome import ChromeDriverManager
from selenium.webdriver.chrome.service import Service

options=Options()
options.add_experimental_option('detach',True)
options.add_experimental_option('excludeSwitches',['enable-logging'])

service = Service(ChromeDriverManager().install())
driver = webdriver.Chrome(service=service, options=options)

my_url = "https://www.silvercarekorea.com/silver/list.php?pagenum=4&addcode=28140&searchkeyword=&orderby=&hashtag=&gubun="
driver.get(my_url)

time.sleep(2)

html = driver.page_source
soup = BeautifulSoup(html, "html.parser")
table = soup.select_one('table.datatable12')

results = []
if table:
    rows = table.find_all('tr')
    for row in rows:
        cols = row.find_all(['td', 'th'])
        row_data = [col.get_text(strip=True) for col in cols]
        # (í•„ìš”í•˜ë‹¤ë©´ ì²« ë²ˆì§¸ trì€ í—¤ë”ë‹ˆê¹Œ ì œì™¸ ë“± ê°€ê³µ ê°€ëŠ¥)
        if row_data:  # ë¹ˆ í–‰ ì œì™¸
            results.append(row_data)

for row in results:
    print(row)

address_keyword = "ì¸ì²œê´‘ì—­ì‹œ"
dong_ends = ["ë™", "ê°€", "ì", "ë©´"]

def smart_split(text):
    # ì¸ì²œê´‘ì—­ì‹œ ìœ„ì¹˜ íƒìƒ‰
    idx_start = text.find(address_keyword)
    if idx_start == -1:
        # íŒ¨í„´ ì—†ìœ¼ë©´ ì „ì²´ë¥¼ ê¸°íƒ€ì •ë³´ë¡œ
        return text, "", ""
    name = text[:idx_start].strip()
    rest = text[idx_start:].strip()
    # 'ë™', 'ê°€', 'ì', 'ë©´' ì¤‘ ê°€ì¥ ë¨¼ì € ë“±ì¥í•˜ëŠ” ìœ„ì¹˜ ì°¾ê¸°
    dong_idx = -1
    for end in dong_ends:
        idx = rest.find(end)
        if idx != -1:
            # +1ë„ ë°˜ë“œì‹œ í¬í•¨(ì˜ˆ: ì†¡í˜„ë™ê¹Œì§€ ì£¼ì†Œë¡œ)
            dong_idx = idx + 1
            break
    if dong_idx == -1:
        # ëª» ì°¾ìœ¼ë©´ ì „ì²´ë¥¼ ì£¼ì†Œë¡œ
        return name, rest, ""
    addr = rest[:dong_idx]
    etc = rest[dong_idx:].strip(" /")  # ë¶ˆí•„ìš”í•œ /, ê³µë°± ì œê±°
    return name, addr, etc

# ì ìš© ì˜ˆì‹œ
for i, row in enumerate(results):
    if i == 0:  # í—¤ë”ìŠ¤í‚µ(ì²« ì¤„)
        continue
    text = row[1]  # ìš”ì–‘ì›ëª…+ì£¼ì†Œ+ê¸°íƒ€í•œ ì…€
    name, addr, etc = smart_split(text)
    print(f"ì‹œì„¤ëª…: {name} | ì£¼ì†Œ: {addr} | ê¸°íƒ€ì •ë³´: {etc}")

try:
    driver.quit()
except:
    pass

# ğŸš¨ ì•ˆì „í•œ WebDriver ì´ˆê¸°í™”
def create_driver():
    options = Options()
    # headless ì œê±°í•´ì„œ ë¸Œë¼ìš°ì € ì°½ ë³´ë©´ì„œ ë””ë²„ê¹…
    options.add_experimental_option('detach', True)
    options.add_experimental_option('excludeSwitches', ['enable-logging'])
    service = Service(ChromeDriverManager().install())
    return webdriver.Chrome(service=service, options=options)

# ìƒˆ ë“œë¼ì´ë²„ ìƒì„±
driver = create_driver()

# ê°„ë‹¨í•œ í…ŒìŠ¤íŠ¸ë¶€í„°
print("=== WebDriver í…ŒìŠ¤íŠ¸ ===")
try:
    test_url = 'https://www.silvercarekorea.com/silver/list.php?pagenum=1&addcode=28140&searchkeyword=&orderby=&hashtag=&gubun='
    driver.get(test_url)
    print("âœ… í˜ì´ì§€ ë¡œë”© ì„±ê³µ!")
    
    time.sleep(3)
    
    # í˜ì´ì§€ ì†ŒìŠ¤ í™•ì¸
    soup = BeautifulSoup(driver.page_source, 'html.parser')
    print(f"âœ… í˜ì´ì§€ íŒŒì‹± ì„±ê³µ! HTML ê¸¸ì´: {len(soup.get_text())}")
    
    # í…Œì´ë¸” ì°¾ê¸°
    all_tables = soup.find_all('table')
    print(f"âœ… í…Œì´ë¸” ê°œìˆ˜: {len(all_tables)}")
    
    # ìš”ì–‘ì› í…ìŠ¤íŠ¸ ì°¾ê¸°
    yoyangwon_texts = soup.find_all(text=lambda text: text and 'ìš”ì–‘ì›' in text)
    print(f"âœ… 'ìš”ì–‘ì›' í¬í•¨ í…ìŠ¤íŠ¸: {len(yoyangwon_texts)}ê°œ")
    
    if len(yoyangwon_texts) > 0:
        print("ì²˜ìŒ 3ê°œ ìš”ì–‘ì›:")
        for i, text in enumerate(yoyangwon_texts[:3]):
            clean_text = text.strip()
            if len(clean_text) > 5:
                print(f"  {i+1}. {clean_text}")
    
except Exception as e:
    print(f"âŒ ì—ëŸ¬ ë°œìƒ: {e}")

# í…ŒìŠ¤íŠ¸ ì™„ë£Œ í›„ ë¸Œë¼ìš°ì €ëŠ” ì—´ì–´ë‘  (detach=True ë•Œë¬¸ì—)
print("\n=== í…ŒìŠ¤íŠ¸ ì™„ë£Œ ===")
print("ë¸Œë¼ìš°ì € ì°½ì—ì„œ í˜ì´ì§€ê°€ ì œëŒ€ë¡œ ë¡œë”©ë˜ì—ˆëŠ”ì§€ ì§ì ‘ í™•ì¸í•´ë³´ì„¸ìš”!")
print("í™•ì¸ í›„ ë‹¤ìŒ ë‹¨ê³„ ì§„í–‰ ê°€ëŠ¥í•©ë‹ˆë‹¤.")

# ë“œë¼ì´ë²„ ìƒì„±
options = Options()
options.add_experimental_option('detach', True)
service = Service(ChromeDriverManager().install())
driver = webdriver.Chrome(service=service, options=options)

# 1í˜ì´ì§€ë§Œ í…ŒìŠ¤íŠ¸
url = 'https://www.silvercarekorea.com/silver/list.php?pagenum=1&addcode=28140&searchkeyword=&orderby=&hashtag=&gubun='
driver.get(url)
time.sleep(3)

soup = BeautifulSoup(driver.page_source, 'html.parser')

# í…Œì´ë¸”ê³¼ ë§í¬ í™•ì¸
all_tables = soup.find_all('table')
print(f"í…Œì´ë¸” ê°œìˆ˜: {len(all_tables)}")

if len(all_tables) > 0:
    table = all_tables[0]  # ì²« ë²ˆì§¸ í…Œì´ë¸”
    rows = table.find_all('tr')
    print(f"í–‰ ê°œìˆ˜: {len(rows)}")
    
    # ë§í¬ê°€ ìˆëŠ”ì§€ í™•ì¸
    links = table.find_all('a')
    print(f"í…Œì´ë¸” ë‚´ ë§í¬ ê°œìˆ˜: {len(links)}")
    
    if len(links) > 0:
        print("âœ… ë§í¬ ì°¾ìŒ! ì „ì²´ ì½”ë“œ ì‹¤í–‰ ê°€ëŠ¥")
        for i, link in enumerate(links[:3]):
            print(f"  {i+1}. {link.get_text()[:20]} â†’ {link.get('href', 'hrefì—†ìŒ')}")
    else:
        print("âŒ ë§í¬ ì—†ìŒ. êµ¬ì¡° ë¶„ì„ í•„ìš”")
else:
    print("âŒ í…Œì´ë¸” ì—†ìŒ. í˜ì´ì§€ êµ¬ì¡° í™•ì¸ í•„ìš”")

try:
    driver.get('https://www.silvercarekorea.com/silver/list.php?pagenum=1&addcode=28140')
    time.sleep(2)
    # ... (í¬ë¡¤ë§ ì½”ë“œ)
except Exception as e:
    print(f"ì—ëŸ¬: {e}")
finally:
    driver.quit()


time.sleep(2)


# ... (Selenium ë“± ì„¸íŒ… ì½”ë“œ)

for row in table.find_all('tr')[1:]:
    cols = row.find_all('td')
    if len(cols) < 2:
        continue
    a_tag = cols[1].find('a')
    # ë°˜ë“œì‹œ 'detail.php'ë§Œ í¬í•¨í•œ ë§í¬ë§Œ!
    if a_tag and a_tag.get('href') and 'detail.php' in a_tag.get('href'):
        detail_url = a_tag['href']
        if not detail_url.startswith('http'):
            if not detail_url.startswith('/'):
                detail_url = '/' + detail_url
            detail_url = 'https://www.silvercarekorea.com' + detail_url
        # ì´í•˜ ìƒì„¸í˜ì´ì§€ ì ‘ê·¼ & ì£¼ì†Œ ì¶”ì¶œì€ ê¸°ì¡´ ë¡œì§ê³¼ ë™ì¼í•˜ê²Œ ì‹¤í–‰


options = Options()
options.add_argument('--headless') # (í•„ìš”ì‹œ ì°½ ì•ˆë„ì›€, ì•ˆí•´ë„ ë¨)
service = Service(ChromeDriverManager().install())
driver = webdriver.Chrome(service=service, options=options)

# í¬ë¡¤ë§í•  URL ë¦¬ìŠ¤íŠ¸
urls = [
    "https://www.silvercarekorea.com/silver/detail.php?uid=6602",
    "https://www.silvercarekorea.com/silver/detail.php?uid=40641",
    "https://www.silvercarekorea.com/silver/detail.php?uid=40640"
    # ì—¬ê¸°ì— í•„ìš”í•œ ë§í¬ ì¶”ê°€ ê°€ëŠ¥
]

headers = {"User-Agent": "Mozilla/5.0"}

def get_value(soup, label):
    for td in soup.find_all('td'):
        td_text = td.get_text(" ", strip=True)
        if td_text.replace(" ", "").startswith(label):
            next_td = td.find_next_sibling('td')
            if next_td:
                return next_td.get_text(strip=True)
            else:
                return td_text.replace(label, "").strip(" :")
    return ""

for i, url in enumerate(urls, 1):
    print(f"\n{'='*15} ë§í¬ {i} ì •ë³´ ({url}) {'='*15}")
    try:
        resp = requests.get(url, headers=headers, timeout=10)
        resp.raise_for_status()  # ìš”ì²­ ì‹¤íŒ¨ ì‹œ ì˜ˆì™¸ ë°œìƒ
        soup = BeautifulSoup(resp.content, 'html.parser')

        ê¸°ê´€ìœ í˜• = get_value(soup, "ê¸°ê´€ìœ í˜•")
        ì£¼ì†Œ = get_value(soup, "ì£¼ì†Œ")
        ì „í™”ë²ˆí˜¸ = get_value(soup, "ì „í™”ë²ˆí˜¸")
        í™ˆí˜ì´ì§€ = get_value(soup, "í™ˆí˜ì´ì§€")
        ì¥ê¸°ìš”ì–‘ê¸°ê´€ì§€ì •ì¼ = get_value(soup, "ì¥ê¸°ìš”ì–‘ê¸°ê´€ì§€ì •ì¼")
        êµí†µí¸ = get_value(soup, "êµí†µí¸")
        ì£¼ì°¨ì‹œì„¤ = get_value(soup, "ì£¼ì°¨ì‹œì„¤")

        print(f"ê¸°ê´€ìœ í˜•: {ê¸°ê´€ìœ í˜•}")
        print(f"ì£¼ì†Œ: {ì£¼ì†Œ}")
        print(f"ì „í™”ë²ˆí˜¸: {ì „í™”ë²ˆí˜¸}")
        print(f"í™ˆí˜ì´ì§€: {í™ˆí˜ì´ì§€}")
        print(f"ì¥ê¸°ìš”ì–‘ê¸°ê´€ì§€ì •ì¼: {ì¥ê¸°ìš”ì–‘ê¸°ê´€ì§€ì •ì¼}")
        print(f"êµí†µí¸: {êµí†µí¸}")
        print(f"ì£¼ì°¨ì‹œì„¤: {ì£¼ì°¨ì‹œì„¤}")
    except requests.RequestException as e:
        print(f"ë§í¬ {i} ìš”ì²­ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")

from selenium import webdriver
from bs4 import BeautifulSoup
import time

driver = webdriver.Chrome()  # í¬ë¡¬ë“œë¼ì´ë²„ í•„ìš”
driver.get('https://www.silvercarekorea.com/silver/detail.php?uid=40640')            # íƒ€ê²Ÿ URL
time.sleep(2)                # ë Œë”ë§ ê¸°ë‹¤ë¦¬ê¸°

html = driver.page_source
soup = BeautifulSoup(html, 'html.parser')
table = soup.find('table', {'class': 'datatable'})
# ì´í•˜ ë™ì¼í•˜ê²Œ rows ë“± ì²˜ë¦¬
driver.quit()

tables = soup.find_all('table', class_='datatable')

# ì¸ë±ìŠ¤ 3, 4, 5 ì„¸ ê°œì˜ í‘œë¥¼ ìˆœì„œëŒ€ë¡œ ì²˜ë¦¬
for idx in [3, 4, 5]:
    #print(f"Table at index {idx}:")
    employee_table = tables[idx]

    rows = employee_table.find_all('tr')
    data = []
    for tr in rows:
        cells = tr.find_all('td')
        row = []
        for td in cells:
            spans = td.find_all('span')
            if spans:
                text = ' '.join([span.get_text(strip=True) for span in spans])
            else:
                text = td.get_text(strip=True)
            row.append(text)
        if row:
            data.append(row)

    for row in data:
        print(row)
    print("\n")  # í‘œë³„ë¡œ êµ¬ë¶„ì„ ìœ„í•´ ë¹ˆ ì¤„ ì¶”ê°€